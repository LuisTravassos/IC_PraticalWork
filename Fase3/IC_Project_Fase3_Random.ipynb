{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb30917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas\n",
    "from scipy.stats import loguniform, uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from SwarmPackagePy import gwo\n",
    "import pyswarms as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57818837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento das imagens\n",
    "def create_street_data(path, street_types, im_size):\n",
    "    images, labels = [], []\n",
    "    streets = [(item, os.path.join(path, item, street)) \n",
    "               for item in street_types \n",
    "               for street in os.listdir(os.path.join(path, item))]\n",
    "    streets_df = pd.DataFrame(streets, columns=['street type', 'image'])\n",
    "    \n",
    "    for _, row in streets_df.iterrows():\n",
    "        img = load_img(row['image'], target_size=(im_size, im_size))\n",
    "        images.append(img_to_array(img))\n",
    "        labels.append(row['street type'])\n",
    "    \n",
    "    return np.array(images, dtype='float32') / 255.0, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb589dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 224\n",
    "\n",
    "street_types = ['clean', 'litter', 'recycle']\n",
    "path = '../Dataset_Menor/'\n",
    "path_test = '../Dataset_Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9f77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streets in each category: clean      50\n",
      "litter     50\n",
      "recycle    50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = create_street_data(path, street_types, im_size)\n",
    "test_images, test_labels = create_street_data(path_test, street_types, im_size)\n",
    "\n",
    "streets_count = pd.value_counts(train_labels)\n",
    "print(\"Streets in each category:\", streets_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32e0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (127, 224, 224, 3)\n",
      "Validation shape: (23, 224, 224, 3)\n",
      "Test shape: (1081, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_images, train_labels_encoded, test_size=0.15, random_state=415)\n",
    "\n",
    "print(f\"Train shape: {train_x.shape}\")\n",
    "print(f\"Validation shape: {val_x.shape}\")\n",
    "print(f\"Test shape: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade4b0dd-31b7-4751-b351-eb85e14ec1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição, compilação e treinamento\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed294b8f-6c83-410d-8630-4c649ff21868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementação da Random Search\n",
    "def create_model(learning_rate, dropout_rate):\n",
    "    top_model = Sequential([\n",
    "        Flatten(input_shape=base_model.output_shape[1:]),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e71d37-006b-4083-860d-99a35857af61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=5, verbose=0)\n",
    "\n",
    "param_distributions = {\n",
    "    'learning_rate': loguniform(1e-5, 0.1),\n",
    "    'dropout_rate': uniform(0.0, 0.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06cbac26-9dab-4a20-96e3-53fa4012bd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x0000022388E98A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 18:01:29,416 - tensorflow - WARNING - 5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x0000022388E98A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000223891F35E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 18:02:58,707 - tensorflow - WARNING - 6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000223891F35E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor: 0.921189 usando {'dropout_rate': 0.18104244959298804, 'learning_rate': 0.0004947772199932062}\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions=param_distributions, \n",
    "                                   n_iter=10,\n",
    "                                   n_jobs=1, \n",
    "                                   cv=3)\n",
    "\n",
    "random_search_result = random_search.fit(train_x, train_y)\n",
    "\n",
    "results = []\n",
    "for params, mean_test_score in zip(random_search_result.cv_results_['params'], random_search_result.cv_results_['mean_test_score']):\n",
    "    results.append({'Hiperparâmetros': params, 'Perda': 1 - mean_test_score})\n",
    "\n",
    "best_learning_rate = random_search_result.best_params_['learning_rate']\n",
    "best_dropout_rate = random_search_result.best_params_['dropout_rate']\n",
    "print(\"Melhor: %f usando %s\" % (random_search_result.best_score_, random_search_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33f508aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 30s 7s/step - loss: 1.3007 - accuracy: 0.3543 - val_loss: 0.8147 - val_accuracy: 0.7391\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.5636 - accuracy: 0.8031 - val_loss: 0.6975 - val_accuracy: 0.7826\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.2686 - accuracy: 0.9764 - val_loss: 0.4971 - val_accuracy: 0.7826\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.1683 - accuracy: 0.9764 - val_loss: 0.4856 - val_accuracy: 0.7826\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.1037 - accuracy: 1.0000 - val_loss: 0.4573 - val_accuracy: 0.8261\n"
     ]
    }
   ],
   "source": [
    "# Definição, compilação e treinamento\n",
    "base_model_output = base_model.output\n",
    "top_layers_output = Flatten()(base_model_output)\n",
    "top_layers_output = Dropout(best_dropout_rate)(top_layers_output)\n",
    "top_layers_output = Dense(3, activation='softmax')(top_layers_output)\n",
    "\n",
    "final_model = Model(inputs=base_model.input, outputs=top_layers_output)\n",
    "\n",
    "final_model.compile(optimizer=Adam(best_learning_rate),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = final_model.fit(train_x, \n",
    "                          train_y, \n",
    "                          epochs=5, \n",
    "                          validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e1165c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor taxa de aprendizado: 0.0004947772199932062\n",
      "Melhor taxa de dropout: 0.18104244959298804\n",
      "34/34 [==============================] - 197s 6s/step - loss: 0.3479 - accuracy: 0.8677\n",
      "\n",
      "Acurácia no teste: 86.77%\n",
      "\n",
      "34/34 [==============================] - 198s 6s/step\n",
      "Matriz de Confusão:\n",
      "[[323  35   7]\n",
      " [ 50 283   8]\n",
      " [ 32  11 332]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean       0.80      0.88      0.84       365\n",
      "      litter       0.86      0.83      0.84       341\n",
      "     recycle       0.96      0.89      0.92       375\n",
      "\n",
      "    accuracy                           0.87      1081\n",
      "   macro avg       0.87      0.87      0.87      1081\n",
      "weighted avg       0.87      0.87      0.87      1081\n",
      "\n",
      "AUC para a classe clean: 0.95\n",
      "AUC para a classe litter: 0.96\n",
      "AUC para a classe recycle: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Avaliação e análise\n",
    "best_results = []\n",
    "best_results.append({'Melhor taxa de aprendizado': best_learning_rate, 'Melhor taxa de dropout': best_dropout_rate})\n",
    "\n",
    "print(f\"Melhor taxa de aprendizado: {best_learning_rate}\")\n",
    "print(f\"Melhor taxa de dropout: {best_dropout_rate}\")\n",
    "\n",
    "test_loss, test_accuracy = final_model.evaluate(test_images, test_labels_encoded, verbose=1)\n",
    "print(f\"\\nAcurácia no teste: {test_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "y_pred_test = final_model.predict(test_images)\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "confusion_mtx = confusion_matrix(test_labels_encoded, y_pred_test_classes)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(confusion_mtx)\n",
    "\n",
    "class_report = classification_report(test_labels_encoded, y_pred_test_classes, target_names=label_encoder.classes_)\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(class_report)\n",
    "\n",
    "y_true_binarized = to_categorical(test_labels_encoded)\n",
    "\n",
    "for i in range(y_true_binarized.shape[1]):\n",
    "    auc_score = roc_auc_score(y_true_binarized[:, i], y_pred_test[:, i])\n",
    "    print(f'AUC para a classe {label_encoder.classes_[i]}: {auc_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f56ea2-70db-4982-86ae-1157720267da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados exportados para o arquivo 'IC_Project_Fase3_Random_Excel.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Armazenamento do Excel e Modelo\n",
    "shapes_data = {\n",
    "    'Conjunto': ['Treino', 'Validação', 'Teste'],\n",
    "    'Formato': [train_x.shape, val_x.shape, test_images.shape]\n",
    "}\n",
    "df_shapes = pd.DataFrame(shapes_data)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_best_results = pd.DataFrame(best_results)\n",
    "\n",
    "df_confusion_mtx = pd.DataFrame(confusion_mtx, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "\n",
    "report_data = classification_report(test_labels_encoded, y_pred_test_classes, target_names=label_encoder.classes_, output_dict=True)\n",
    "df_class_report = pd.DataFrame(report_data).transpose()\n",
    "\n",
    "auc_scores = {label_encoder.classes_[i]: roc_auc_score(y_true_binarized[:, i], y_pred_test[:, i]) for i in range(y_true_binarized.shape[1])}\n",
    "df_auc_scores = pd.DataFrame(list(auc_scores.items()), columns=['Classe', 'AUC'])\n",
    "\n",
    "with pd.ExcelWriter('IC_Project_Fase3_Random_Excel.xlsx') as writer:\n",
    "    df_shapes.to_excel(writer, sheet_name='Formas dos Conjuntos', index=False)\n",
    "    df_results.to_excel(writer, sheet_name='Resultados Otimizacao', index=False)\n",
    "    df_best_results.to_excel(writer, sheet_name='Melhor Resultado', index=False)\n",
    "    df_confusion_mtx.to_excel(writer, sheet_name='Matriz de Confusão')\n",
    "    df_class_report.to_excel(writer, sheet_name='Relatório de Classificação')\n",
    "    df_auc_scores.to_excel(writer, sheet_name='AUC por Classe', index=False)\n",
    "\n",
    "print(\"Resultados exportados para o arquivo 'IC_Project_Fase3_Random_Excel.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4cbb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo com sucesso em: IC_Project_Fase3_Random_SaveModel.h5\n"
     ]
    }
   ],
   "source": [
    "model_save_path = 'IC_Project_Fase3_Random_SaveModel.h5'\n",
    "\n",
    "final_model.save(model_save_path)\n",
    "print(f\"Modelo salvo com sucesso em: {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
