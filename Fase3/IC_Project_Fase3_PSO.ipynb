{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb30917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas\n",
    "from scipy.stats import loguniform, uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from SwarmPackagePy import gwo\n",
    "import pyswarms as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57818837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento das imagens\n",
    "def create_street_data(path, street_types, im_size):\n",
    "    images, labels = [], []\n",
    "    streets = [(item, os.path.join(path, item, street)) \n",
    "               for item in street_types \n",
    "               for street in os.listdir(os.path.join(path, item))]\n",
    "    streets_df = pd.DataFrame(streets, columns=['street type', 'image'])\n",
    "    \n",
    "    for _, row in streets_df.iterrows():\n",
    "        img = load_img(row['image'], target_size=(im_size, im_size))\n",
    "        images.append(img_to_array(img))\n",
    "        labels.append(row['street type'])\n",
    "    \n",
    "    return np.array(images, dtype='float32') / 255.0, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb589dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 224\n",
    "\n",
    "street_types = ['clean', 'litter', 'recycle']\n",
    "path = '../Dataset_Menor/'\n",
    "path_test = '../Dataset_Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9f77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streets in each category: clean      50\n",
      "litter     50\n",
      "recycle    50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = create_street_data(path, street_types, im_size)\n",
    "test_images, test_labels = create_street_data(path_test, street_types, im_size)\n",
    "\n",
    "streets_count = pd.value_counts(train_labels)\n",
    "print(\"Streets in each category:\", streets_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32e0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (127, 224, 224, 3)\n",
      "Validation shape: (23, 224, 224, 3)\n",
      "Test shape: (1081, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_images, train_labels_encoded, test_size=0.15, random_state=415)\n",
    "\n",
    "print(f\"Train shape: {train_x.shape}\")\n",
    "print(f\"Validation shape: {val_x.shape}\")\n",
    "print(f\"Test shape: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "385a2ce5-bd74-4172-9b12-b20708810066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definição, compilação e treinamento\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed294b8f-6c83-410d-8630-4c649ff21868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementação do PSO\n",
    "def train_top_layer(hyperparameters, train_x, train_y, val_x, val_y, base_model):\n",
    "    learning_rate, dropout_rate = hyperparameters\n",
    "    \n",
    "    top_model = Sequential([\n",
    "        Flatten(input_shape=base_model.output_shape[1:]),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(train_x, \n",
    "                        train_y, \n",
    "                        epochs=5, \n",
    "                        validation_data=(val_x, val_y), \n",
    "                        verbose=0)\n",
    "    \n",
    "    validation_loss, validation_accuracy = model.evaluate(val_x, val_y, verbose=0)\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72e71d37-006b-4083-860d-99a35857af61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fitness_function(x, train_x, train_y, val_x, val_y, base_model):\n",
    "    \n",
    "    n_particles = x.shape[0]\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i in range(n_particles):\n",
    "        hyperparameters = x[i]\n",
    "        loss = train_top_layer(hyperparameters, train_x, train_y, val_x, val_y, base_model)\n",
    "        losses.append(loss)\n",
    "        results.append({'Hiperparâmetros': hyperparameters, 'Perda': loss})\n",
    "        \n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06cbac26-9dab-4a20-96e3-53fa4012bd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 00:14:49,088 - pyswarms.single.global_best - INFO - Optimize for 5 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%|██████████|5/5, best_cost=0.591\n",
      "2023-12-12 00:58:58,086 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.5905982851982117, best pos: [1.66970166e-04 4.01440269e-01]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "bounds = [(0.0001, 0.1), (0.0, 0.5)]\n",
    "options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=5, \n",
    "                                    dimensions=2, \n",
    "                                    options=options, \n",
    "                                    bounds=bounds)\n",
    "\n",
    "cost, best_pos = optimizer.optimize(fitness_function, \n",
    "                                    iters=5, \n",
    "                                    train_x=train_x, \n",
    "                                    train_y=train_y, \n",
    "                                    val_x=val_x, \n",
    "                                    val_y=val_y,\n",
    "                                    base_model=base_model)\n",
    "\n",
    "best_learning_rate, best_dropout_rate = best_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33f508aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1.2365 - accuracy: 0.4016 - val_loss: 1.0523 - val_accuracy: 0.2609\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 20s 5s/step - loss: 1.0483 - accuracy: 0.4961 - val_loss: 0.9145 - val_accuracy: 0.6087\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.7623 - accuracy: 0.6614 - val_loss: 0.8000 - val_accuracy: 0.6087\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 21s 6s/step - loss: 0.6032 - accuracy: 0.7638 - val_loss: 0.7118 - val_accuracy: 0.7826\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 22s 6s/step - loss: 0.4910 - accuracy: 0.8110 - val_loss: 0.7047 - val_accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "# Definição, compilação e treinamento\n",
    "base_model_output = base_model.output\n",
    "top_layers_output = Flatten()(base_model_output)\n",
    "top_layers_output = Dropout(best_dropout_rate)(top_layers_output)\n",
    "top_layers_output = Dense(3, activation='softmax')(top_layers_output)\n",
    "\n",
    "final_model = Model(inputs=base_model.input, outputs=top_layers_output)\n",
    "\n",
    "final_model.compile(optimizer=Adam(best_learning_rate),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = final_model.fit(train_x, \n",
    "                          train_y, \n",
    "                          epochs=5, \n",
    "                          validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e1165c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor taxa de aprendizado: 0.00016697016593511955\n",
      "Melhor taxa de dropout: 0.40144026943574496\n",
      "34/34 [==============================] - 153s 4s/step - loss: 0.5769 - accuracy: 0.7946\n",
      "\n",
      "Acurácia no teste: 79.46%\n",
      "\n",
      "34/34 [==============================] - 154s 5s/step\n",
      "Matriz de Confusão:\n",
      "[[336  24   5]\n",
      " [101 234   6]\n",
      " [ 79   7 289]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean       0.65      0.92      0.76       365\n",
      "      litter       0.88      0.69      0.77       341\n",
      "     recycle       0.96      0.77      0.86       375\n",
      "\n",
      "    accuracy                           0.79      1081\n",
      "   macro avg       0.83      0.79      0.80      1081\n",
      "weighted avg       0.83      0.79      0.80      1081\n",
      "\n",
      "AUC para a classe clean: 0.93\n",
      "AUC para a classe litter: 0.95\n",
      "AUC para a classe recycle: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Avaliação e análise\n",
    "best_results = []\n",
    "best_results.append({'Melhor taxa de aprendizado': best_learning_rate, 'Melhor taxa de dropout': best_dropout_rate})\n",
    "\n",
    "print(f\"Melhor taxa de aprendizado: {best_learning_rate}\")\n",
    "print(f\"Melhor taxa de dropout: {best_dropout_rate}\")\n",
    "\n",
    "test_loss, test_accuracy = final_model.evaluate(test_images, test_labels_encoded, verbose=1)\n",
    "print(f\"\\nAcurácia no teste: {test_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "y_pred_test = final_model.predict(test_images)\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "confusion_mtx = confusion_matrix(test_labels_encoded, y_pred_test_classes)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(confusion_mtx)\n",
    "\n",
    "class_report = classification_report(test_labels_encoded, y_pred_test_classes, target_names=label_encoder.classes_)\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(class_report)\n",
    "\n",
    "y_true_binarized = to_categorical(test_labels_encoded)\n",
    "\n",
    "for i in range(y_true_binarized.shape[1]):\n",
    "    auc_score = roc_auc_score(y_true_binarized[:, i], y_pred_test[:, i])\n",
    "    print(f'AUC para a classe {label_encoder.classes_[i]}: {auc_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7f56ea2-70db-4982-86ae-1157720267da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados exportados para o arquivo 'IC_Project_Fase3_PSO_Excel.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Armazenamento do Excel e Modelo\n",
    "shapes_data = {\n",
    "    'Conjunto': ['Treino', 'Validação', 'Teste'],\n",
    "    'Formato': [train_x.shape, val_x.shape, test_images.shape]\n",
    "}\n",
    "df_shapes = pd.DataFrame(shapes_data)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_best_results = pd.DataFrame(best_results)\n",
    "\n",
    "df_confusion_mtx = pd.DataFrame(confusion_mtx, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "\n",
    "report_data = classification_report(test_labels_encoded, y_pred_test_classes, target_names=label_encoder.classes_, output_dict=True)\n",
    "df_class_report = pd.DataFrame(report_data).transpose()\n",
    "\n",
    "auc_scores = {label_encoder.classes_[i]: roc_auc_score(y_true_binarized[:, i], y_pred_test[:, i]) for i in range(y_true_binarized.shape[1])}\n",
    "df_auc_scores = pd.DataFrame(list(auc_scores.items()), columns=['Classe', 'AUC'])\n",
    "\n",
    "with pd.ExcelWriter('IC_Project_Fase3_PSO_Excel.xlsx') as writer:\n",
    "    df_shapes.to_excel(writer, sheet_name='Formas dos Conjuntos', index=False)\n",
    "    df_results.to_excel(writer, sheet_name='Resultados Otimizacao', index=False)\n",
    "    df_best_results.to_excel(writer, sheet_name='Melhor Resultado', index=False)\n",
    "    df_confusion_mtx.to_excel(writer, sheet_name='Matriz de Confusão')\n",
    "    df_class_report.to_excel(writer, sheet_name='Relatório de Classificação')\n",
    "    df_auc_scores.to_excel(writer, sheet_name='AUC por Classe', index=False)\n",
    "\n",
    "print(\"Resultados exportados para o arquivo 'IC_Project_Fase3_PSO_Excel.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb4cbb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo com sucesso em: IC_Project_Fase3_PSO_SaveModel.h5\n"
     ]
    }
   ],
   "source": [
    "model_save_path = 'IC_Project_Fase3_PSO_SaveModel.h5'\n",
    "\n",
    "final_model.save(model_save_path)\n",
    "print(f\"Modelo salvo com sucesso em: {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
