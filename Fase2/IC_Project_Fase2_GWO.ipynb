{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb30917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from SwarmPackagePy import gwo\n",
    "\n",
    "import pyswarms as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57818837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_street_data(path, street_types, im_size):\n",
    "    images, labels = [], []\n",
    "    streets = [(item, os.path.join(path, item, street)) \n",
    "               for item in street_types \n",
    "               for street in os.listdir(os.path.join(path, item))]\n",
    "    streets_df = pd.DataFrame(streets, columns=['street type', 'image'])\n",
    "    \n",
    "    for _, row in streets_df.iterrows():\n",
    "        img = load_img(row['image'], target_size=(im_size, im_size))\n",
    "        images.append(img_to_array(img))\n",
    "        labels.append(row['street type'])\n",
    "    \n",
    "    return np.array(images, dtype='float32') / 255.0, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb589dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o tamanho da imagem e os tipos de rua\n",
    "im_size = 350  # Exemplo de tamanho de imagem\n",
    "\n",
    "\"\"\"\n",
    "street_types = ['Apple', 'Banana', 'Cocos']\n",
    "path = '../DatasetFruits/'\n",
    "path_test = '../DatasetFruits_Test/'\n",
    "\"\"\"\n",
    "\n",
    "street_types = ['clean', 'litter', 'recycle']\n",
    "path = '../Dataset/'\n",
    "path_test = '../Dataset_Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9f77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streets in each category: recycle    1675\n",
      "clean      1625\n",
      "litter     1505\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Criando dados de treino, validação e teste\n",
    "train_images, train_labels = create_street_data(path, street_types, im_size)\n",
    "test_images, test_labels = create_street_data(path_test, street_types, im_size)\n",
    "\n",
    "# Contando as ocorrências de cada tipo de rua e imprimindo\n",
    "streets_count = pd.value_counts(train_labels)\n",
    "print(\"Streets in each category:\", streets_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32e0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3844, 350, 350, 3)\n",
      "Validation shape: (961, 350, 350, 3)\n",
      "Test shape: (600, 350, 350, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convertendo etiquetas para valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Embaralhando e dividindo os dados de treino e validação\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_images, train_labels_encoded, test_size=0.2, random_state=415)\n",
    "\n",
    "# Imprimindo as formas dos conjuntos de dados\n",
    "print(f\"Train shape: {train_x.shape}\")\n",
    "print(f\"Validation shape: {val_x.shape}\")\n",
    "print(f\"Test shape: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b829c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(hyperparameters, train_x, train_y, val_x, val_y):\n",
    "    learning_rate, dropout_rate = hyperparameters  \n",
    "    im_size = train_x.shape[1]\n",
    "    \n",
    "    # Definindo o modelo da rede neural\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', input_shape=(im_size, im_size, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dropout(dropout_rate),  # Usar a variável desempacotada\n",
    "        Dense(len(street_types), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_x, \n",
    "              train_y, \n",
    "              epochs=5, \n",
    "              validation_data=(val_x, val_y), \n",
    "              verbose=0)\n",
    "    \n",
    "    validation_loss, validation_accuracy = model.evaluate(val_x, val_y, verbose=0)\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddddfe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina a função de fitness que será usada pelo GWO\n",
    "def fitness_function(hyperparameters):\n",
    "    loss = train_network(hyperparameters, train_x, train_y, val_x, val_y)\n",
    "    print(f\"Particle: {hyperparameters}, Loss: {loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a03f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle: [0.00954004 0.14398427], Loss: 1.0972198247909546\n",
      "Particle: [0.02901177 0.06104907], Loss: 1.0975923538208008\n",
      "Particle: [0.05643835 0.11398582], Loss: 1.0956742763519287\n",
      "Particle: [0.00070295 0.00083243], Loss: 0.2559313178062439\n",
      "Particle: [0.08718363 0.02419298], Loss: 1.10580575466156\n",
      "Particle: [0.0001     0.08653952], Loss: 0.2087739109992981\n",
      "Particle: [0.00616273 0.02972102], Loss: 0.6086217164993286\n",
      "Particle: [0.0001     0.04749157], Loss: 0.33499205112457275\n",
      "Particle: [0.02618937 0.19308755], Loss: 1.095405101776123\n",
      "Particle: [0.0001   0.070102], Loss: 0.1966089904308319\n",
      "Particle: [0.0001   0.070102], Loss: 0.2312220185995102\n",
      "Particle: [0.00070295 0.00083243], Loss: 0.21457263827323914\n",
      "Particle: [0.0001     0.04428281], Loss: 0.26600903272628784\n",
      "Particle: [1.00000000e-04 1.19475957e-01], Loss: 0.22442041337490082\n",
      "Particle: [0.0001     0.07221296], Loss: 0.22926072776317596\n",
      "Particle: [0.01803382 0.        ], Loss: 1.0957415103912354\n",
      "Particle: [0.00016271 0.08452835], Loss: 0.20167149603366852\n",
      "Particle: [0.00016271 0.08452835], Loss: 0.24277138710021973\n",
      "Particle: [0.00070295 0.00083243], Loss: 0.2437385469675064\n",
      "Particle: [0.00011411 0.08599611], Loss: 0.27188703417778015\n",
      "Particle: [0.00012226 0.08631445], Loss: 0.24065832793712616\n",
      "Particle: [0.0001     0.05865396], Loss: 0.21935924887657166\n",
      "Particle: [1.00000000e-04 1.50127239e-01], Loss: 0.2250315397977829\n",
      "Particle: [0.00011958 0.0677735 ], Loss: 0.20188283920288086\n",
      "Particle: [0.00011958 0.0677735 ], Loss: 0.2076943814754486\n",
      "Particle: [0.00016271 0.08452835], Loss: 0.2369014322757721\n",
      "Particle: [0.00011579 0.10204775], Loss: 0.23411589860916138\n",
      "Particle: [0.0001149  0.09861804], Loss: 0.21118997037410736\n",
      "Particle: [0.0001     0.02361499], Loss: 0.2076566368341446\n",
      "Particle: [0.00011183 0.10241372], Loss: 0.2270757257938385\n",
      "Particle: [0.00012542 0.1010612 ], Loss: 0.18673279881477356\n",
      "Particle: [0.00012542 0.1010612 ], Loss: 0.1938813477754593\n",
      "Particle: [0.00011958 0.0677735 ], Loss: 0.22689464688301086\n",
      "Particle: [0.00012771 0.07512076], Loss: 0.20991547405719757\n",
      "Particle: [0.0001     0.08303738], Loss: 0.21095359325408936\n",
      "Particle: [0.0001067  0.07966981], Loss: 0.2187715619802475\n",
      "Particle: [0.0001036  0.07295673], Loss: 0.21270829439163208\n",
      "Particle: [0.00011001 0.0753221 ], Loss: 0.22510026395320892\n",
      "Particle: [0.00012771 0.07512076], Loss: 0.20832517743110657\n",
      "Particle: [0.00012542 0.1010612 ], Loss: 0.19991427659988403\n",
      "Particle: [0.00012771 0.07512076], Loss: 0.20427636802196503\n",
      "Particle: [0.0001     0.08303738], Loss: 0.23352856934070587\n",
      "Particle: [0.0001067  0.07966981], Loss: 0.23337368667125702\n",
      "Particle: [0.0001036  0.07295673], Loss: 0.20135226845741272\n",
      "Particle: [0.00011001 0.0753221 ], Loss: 0.22343511879444122\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros do GWO\n",
    "n_agents = 5\n",
    "n_iterations = 5\n",
    "dimensions = 2\n",
    "lower_bound = [0.0001, 0.0]  # Limites inferiores para taxa de aprendizado e dropout\n",
    "upper_bound = [0.1, 0.5]     # Limites superiores para taxa de aprendizado e dropout\n",
    "\n",
    "# Inicializar e executar o otimizador GWO\n",
    "optimizer = gwo(n_agents, \n",
    "                fitness_function, \n",
    "                lower_bound, \n",
    "                upper_bound, \n",
    "                dimensions, \n",
    "                n_iterations)\n",
    "\n",
    "# Desempacotando os melhores hiperparâmetros encontrados\n",
    "best_learning_rate, best_dropout_rate = optimizer.get_Gbest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e26aa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "121/121 [==============================] - 147s 1s/step - loss: 1.5913 - accuracy: 0.6262 - val_loss: 0.8296 - val_accuracy: 0.6670\n",
      "Epoch 2/5\n",
      "121/121 [==============================] - 143s 1s/step - loss: 0.3019 - accuracy: 0.8980 - val_loss: 0.3075 - val_accuracy: 0.8980\n",
      "Epoch 3/5\n",
      "121/121 [==============================] - 143s 1s/step - loss: 0.1211 - accuracy: 0.9732 - val_loss: 0.3710 - val_accuracy: 0.8720\n",
      "Epoch 4/5\n",
      "121/121 [==============================] - 143s 1s/step - loss: 0.0694 - accuracy: 0.9878 - val_loss: 0.2229 - val_accuracy: 0.9334\n",
      "Epoch 5/5\n",
      "121/121 [==============================] - 142s 1s/step - loss: 0.0378 - accuracy: 0.9953 - val_loss: 0.2191 - val_accuracy: 0.9282\n"
     ]
    }
   ],
   "source": [
    "final_model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(im_size, im_size, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dropout(best_dropout_rate),\n",
    "\n",
    "    Dense(len(street_types), activation='softmax')    \n",
    "])\n",
    "\n",
    "final_model.compile(optimizer=Adam(best_learning_rate),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = final_model.fit(train_x, \n",
    "                          train_y, \n",
    "                          epochs=5,\n",
    "                          validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1165c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 8s 287ms/step - loss: 0.2507 - accuracy: 0.9167\n",
      "19/19 [==============================] - 4s 195ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = final_model.evaluate(test_images, test_labels_encoded, verbose=1);\n",
    "\n",
    "y_pred_test = final_model.predict(test_images);\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1);\n",
    "\n",
    "y_true = test_labels_encoded;\n",
    "y_true_binarized = label_binarize(test_labels_encoded, classes=np.unique(test_labels_encoded));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb4cbb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 0.00012542080373561626\n",
      "Best dropout rate: 0.10106120443028664\n",
      "\n",
      "Test accuracy: 91.67%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[176   8  16]\n",
      " [ 14 184   2]\n",
      " [  9   1 190]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clean       0.88      0.88      0.88       200\n",
      "      litter       0.95      0.92      0.94       200\n",
      "     recycle       0.91      0.95      0.93       200\n",
      "\n",
      "    accuracy                           0.92       600\n",
      "   macro avg       0.92      0.92      0.92       600\n",
      "weighted avg       0.92      0.92      0.92       600\n",
      "\n",
      "\n",
      "\n",
      "AUC for class clean: 0.97\n",
      "AUC for class litter: 0.98\n",
      "AUC for class recycle: 0.99\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best learning rate: {best_learning_rate}\")\n",
    "print(f\"Best dropout rate: {best_dropout_rate}\")\n",
    "\n",
    "# Avaliação do modelo nos dados de teste\n",
    "print(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\" + \"\\n\");\n",
    "\n",
    "# Matriz de confusão\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_test_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)\n",
    "\n",
    "# Relatório de classificação\n",
    "class_report = classification_report(y_true, y_pred_test_classes, target_names=label_encoder.classes_)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Calculando o AUC para cada classe\n",
    "for i in range(y_true_binarized.shape[1]):  # Percorre cada classe\n",
    "    auc_score = roc_auc_score(y_true_binarized[:, i], y_pred_test[:, i])\n",
    "    print(f'AUC for class {label_encoder.classes_[i]}: {auc_score:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
