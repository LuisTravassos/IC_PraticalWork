{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb30917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pyswarms as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57818837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_street_data(path, street_types, im_size):\n",
    "    images, labels = [], []\n",
    "    streets = [(item, os.path.join(path, item, street)) \n",
    "               for item in street_types \n",
    "               for street in os.listdir(os.path.join(path, item))]\n",
    "    streets_df = pd.DataFrame(streets, columns=['street type', 'image'])\n",
    "    \n",
    "    for _, row in streets_df.iterrows():\n",
    "        img = load_img(row['image'], target_size=(im_size, im_size))\n",
    "        images.append(img_to_array(img))\n",
    "        labels.append(row['street type'])\n",
    "    \n",
    "    return np.array(images, dtype='float32') / 255.0, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb589dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o tamanho da imagem e os tipos de rua\n",
    "im_size = 350  # Exemplo de tamanho de imagem\n",
    "street_types = ['clean', 'litter', 'recycle']\n",
    "path = '../Dataset/'\n",
    "path_test = '../Dataset_Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9f77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streets in each category: recycle    1675\n",
      "clean      1625\n",
      "litter     1505\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Criando dados de treino, validação e teste\n",
    "train_images, train_labels = create_street_data(path, street_types, im_size)\n",
    "test_images, test_labels = create_street_data(path_test, street_types, im_size)\n",
    "\n",
    "# Contando as ocorrências de cada tipo de rua e imprimindo\n",
    "streets_count = pd.value_counts(train_labels)\n",
    "print(\"Streets in each category:\", streets_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32e0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3844, 350, 350, 3)\n",
      "Validation shape: (961, 350, 350, 3)\n",
      "Test shape: (600, 350, 350, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convertendo etiquetas para valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Embaralhando e dividindo os dados de treino e validação\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_images, train_labels_encoded, test_size=0.2, random_state=415)\n",
    "\n",
    "# Imprimindo as formas dos conjuntos de dados\n",
    "print(f\"Train shape: {train_x.shape}\")\n",
    "print(f\"Validation shape: {val_x.shape}\")\n",
    "print(f\"Test shape: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b829c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina as funções de treinamento de rede e fitness para o PSO\n",
    "def train_network(hyperparameters, train_x, train_y, val_x, val_y):\n",
    "    \"\"\"learning_rate, dropout_rate = hyperparameters\n",
    "    # Definindo o modelo da rede neural convolucional\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(im_size, im_size, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(len(street_types), activation='softmax')   \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_x, \n",
    "              train_y, \n",
    "              epochs=5, \n",
    "              batch_size=64,\n",
    "              validation_data=(val_x, val_y), \n",
    "              verbose=0)\n",
    "    \n",
    "    validation_loss, validation_accuracy = model.evaluate(val_x, val_y, verbose=0)\"\"\"\n",
    "    validation_loss = 0.1\n",
    "    \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c19740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function(x, train_x, train_y, val_x, val_y):\n",
    "    n_particles = x.shape[0]\n",
    "    losses = []\n",
    "    for i in range(n_particles):\n",
    "        # x[i] contém os hiperparâmetros para a i-ésima partícula\n",
    "        hyperparameters = x[i]\n",
    "        loss = train_network(hyperparameters, train_x, train_y, val_x, val_y)\n",
    "        losses.append(loss)\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14a03f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 16:58:39,724 - pyswarms.single.global_best - INFO - Optimize for 5 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%|██████████|5/5, best_cost=0.1\n",
      "2023-11-07 16:58:39,755 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.1, best pos: [7.15676915e-05 3.96627299e-01]\n"
     ]
    }
   ],
   "source": [
    "# Inicializando o otimizador GlobalBestPSO do pyswarms\n",
    "bounds = [(0.0001, 0.1), (0.0, 0.5)]  # (min_learning_rate, max_learning_rate), (min_dropout_rate, max_dropout_rate)\n",
    "options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=5, \n",
    "                                    dimensions=2, \n",
    "                                    options=options, \n",
    "                                    bounds=bounds)\n",
    "\n",
    "# Executando o PSO para encontrar os melhores hiperparâmetros\n",
    "cost, best_pos = optimizer.optimize(fitness_function, \n",
    "                                    iters=5, \n",
    "                                    train_x=train_x, \n",
    "                                    train_y=train_y, \n",
    "                                    val_x=val_x, \n",
    "                                    val_y=val_y)\n",
    "\n",
    "best_learning_rate, best_dropout_rate = best_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e26aa68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m\n\u001b[0;32m      1\u001b[0m final_model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      2\u001b[0m     Conv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(im_size, im_size, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m      3\u001b[0m     MaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     Dense(\u001b[38;5;28mlen\u001b[39m(street_types), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[0;32m     12\u001b[0m ])\n\u001b[0;32m     14\u001b[0m final_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mbest_learning_rate),\n\u001b[0;32m     15\u001b[0m                     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m                     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m history \u001b[38;5;241m=\u001b[39m final_model\u001b[38;5;241m.\u001b[39mfit(train_x, \n\u001b[0;32m     19\u001b[0m                           train_y, \n\u001b[0;32m     20\u001b[0m                           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m     21\u001b[0m                           batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[1;32m---> 22\u001b[0m                           validation_data\u001b[38;5;241m=\u001b[39m(\u001b[43mtest_x\u001b[49m, test_y))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_x' is not defined"
     ]
    }
   ],
   "source": [
    "final_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(im_size, im_size, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(best_dropout_rate),\n",
    "    Dense(len(street_types), activation='softmax')   \n",
    "])\n",
    "\n",
    "final_model.compile(optimizer=Adam(learning_rate=best_learning_rate),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = final_model.fit(train_x, \n",
    "                          train_y, \n",
    "                          epochs=5, \n",
    "                          batch_size=32, \n",
    "                          validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do modelo nos dados de teste\n",
    "test_loss, test_accuracy = final_model.evaluate(test_x, test_y, verbose=1)\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "final_model.save('final_model.h5')\n",
    "\n",
    "# Previsões no conjunto de teste\n",
    "y_pred_test = final_model.predict(test_x)\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
    "y_true = np.argmax(test_y, axis=1)  # Supondo que 'test_y' é one-hot encoded\n",
    "\n",
    "# Matriz de confusão\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_test_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)\n",
    "\n",
    "# Relatório de classificação\n",
    "class_report = classification_report(y_true, y_pred_test_classes, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Cálculo de AUC para cada classe\n",
    "y_probs = final_model.predict(test_x)  # Supondo que existam várias classes\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    auc_score = roc_auc_score(test_y[:, i], y_probs[:, i])\n",
    "    print(f\"AUC for {class_name}: {auc_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
